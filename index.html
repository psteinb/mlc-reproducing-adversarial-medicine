<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><title>Blind Adversarial Attacks Against Medical Deep Learning Systems</title><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui" name="viewport"><link href="reveal.js/css/reveal.css" rel="stylesheet"><link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme"><!--This CSS is generated by the Asciidoctor-Reveal.js converter to further integrate AsciiDoc's existing semantic with Reveal.js--><style type="text/css">.reveal div.right {
  float: right;
}

/* callouts */
.conum[data-value] {display:inline-block;color:#fff!important;background-color:rgba(50,150,50,.8);-webkit-border-radius:100px;border-radius:100px;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]:after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}</style><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.3.0/css/font-awesome.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
tex2jax: {
  inlineMath: [["\\(", "\\)"]],
  displayMath: [["\\[", "\\]"]],
  ignoreClass: "nostem|nolatexmath"
},
asciimath2jax: {
  delimiters: [["\\$", "\\$"]],
  ignoreClass: "nostem|noasciimath"
},
TeX: { equationNumbers: { autoNumber: "none" } }
});</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.4.0/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script><link href="reveal.js/lib/css/zenburn.css" rel="stylesheet"><script>var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? "reveal.js/css/print/pdf.css" : "reveal.js/css/print/paper.css";
document.getElementsByTagName( 'head' )[0].appendChild( link );</script><!--[if lt IE 9]><script src="reveal.js/lib/js/html5shiv.js"></script><![endif]--><link rel="stylesheet" href="custom.css"></head><body><div class="reveal"><div class="slides"><section class="title" data-state="title"><h1>Blind Adversarial Attacks Against Medical Deep Learning Systems</h1><div class="preamble"><div class="paragraph"><p><a href="mailto:steinbach@extern.mpi-cbg.de">Peter Steinbach</a> (<a href="https://www.mpi-cbg.de">MPI CBG</a>, <a href="https://www.scionics.de">Scionics</a>)<br>
2nd MLC Workshop May 16, 2019<br>
Helmholtz-Zentrum Dresden-Rossendorf, Germany<br>
<a href="https://twitter.com/psteinb_"><span class="icon"><i class="fa fa-twitter"></i></span> psteinb_</a> <a href="https://github.com/psteinb"><span class="icon"><i class="fa fa-github"></i></span> psteinb</a><br></p></div></div></section>
<section><section id="_introducing_adversarial_examples"><h2>Introducing Adversarial Examples</h2><div class="paragraph"><p><span class="icon"><i class="fa fa-question-circle fa-5x"></i></span></p></div></section><section id="_supervised_deep_learning_classifiers"><h2>Supervised Deep Learning Classifiers</h2><div class="paragraph"><p><span class="image"><img src="images/dl-process.png" alt="dl process" width="75%"></span></p></div>
<div class="paragraph"><p>From <a href="#MapRBlog">[MapRBlog]</a></p></div>
<aside class="notes"><div class="ulist"><ul><li><p>classification (accuracy better than human)</p></li><li><p>large training/test set</p></li><li><p>data set = good balance of classes</p></li></ul></div></aside></section><section id="_what_are_adversarial_examples"><h2>What are Adversarial Examples?</h2><div class="paragraph"><p><span class="image"><img src="images/advex-explained-1-nogradient.png" alt="advex explained 1 nogradient" width="90%"></span></p></div>
<div class="paragraph"><p>From <a href="#Goodfellow14">[Goodfellow14]</a> (based on <a href="#Szegedy14">[Szegedy14]</a>)</p></div></section><section id="_fooling_by_printing"><h2>Fooling by printing</h2><div class="paragraph"><p><span class="image"><img src="images/advex-explained-2.png" alt="advex explained 2" width="90%"></span></p></div>
<div class="paragraph"><p>From <a href="#Kurakin16">[Kurakin16]</a></p></div></section><section id="_fooling_by_backdoors"><h2>Fooling by backdoors</h2><div class="paragraph"><p><span class="image"><img src="images/badnet-without-caption.png" alt="badnet without caption" width="50%"></span></p></div>
<div class="paragraph"><p>From <a href="#Gu17">[Gu17]</a></p></div></section><section id="_consequences"><h2>Consequences?</h2><div class="ulist"><ul><li><p>security of autonomous driving</p></li><li><p>security of voice recognition systems (Siri, Alexa, &#8230;&#8203;)</p></li><li><p>product quality of online tools</p></li><li><p>trust in automated decision tools</p></li><li><p>trust in scientific classifiers</p></li></ul></div>
<div class="admonitionblock warning"><table><tr><td class="icon"><i class="fa fa-warning" title="Warning"></i></td><td class="content"><span class="XL-text">Affects any deployed ML system!</span></td></tr></table></div></section></section>
<section><section id="_adversarial_attacks_against_medical_deep_learning_systems"><h2>Adversarial Attacks Against Medical Deep Learning Systems</h2></section><section id="_doctors_appointment"><h2>Doctor&#8217;s Appointment</h2><div class="paragraph"><p><span class="icon"><i class="fa fa-user-md fa-5x"></i></span></p></div></section><section id="_measurement"><h2>Measurement</h2><div class="paragraph"><p><span class="icon"><i class="fa fa-heartbeat fa-5x"></i></span></p></div></section><section id="_diagnosis"><h2>Diagnosis</h2><div class="paragraph"><p><span class="icon"><i class="fa fa-medkit fa-5x"></i></span> &nbsp; or &nbsp; <span class="icon"><i class="fa fa-heart fa-5x"></i></span></p></div></section><section id="_adversarial_attacks_against_medical_deep_learning_systems_2"><h2>Adversarial Attacks Against Medical Deep Learning Systems</h2><div class="paragraph"><p><span class="XL-text">Samuel G. Finlayson, Hyung Won Chung, Isaac S. Kohane, Andrew L. Beam (HMS+Harvard+MIT)</span><br></p></div>
<div class="paragraph"><p>Science, 22 Mar 2019, Vol. 363, Issue 6433, pp. 1287-1289, <a href="https://doi.org/10.1126/science.aaw4399">10.1126/science.aaw4399</a></p></div>
<div class="quoteblock"><blockquote><div class="paragraph"><p>The prospect of improving healthcare and medicine with the use
of deep learning is truly exciting. [&#8230;&#8203;]
it seems inevitable that medical deep learning algorithms will become entrenched in the already multi-billion dollar medical information technology industry.</p></div></blockquote></div>
<aside class="notes"><div class="ulist"><ul><li><p>published in Science</p></li><li><p>reproduced it</p></li></ul></div></aside></section><section id="_conclusion" class="XL-text"><h2>Conclusion</h2><div class="quoteblock"><blockquote><div class="paragraph"><p>In this work, we have outlined the systemic and technological reasons that cause adversarial examples to pose a disproportion-
ately large threat in the medical domain, and provided examples of how such attacks may be executed.</p></div></blockquote><div class="attribution">&#8212; Finlayson et al. "Adversarial Attacks Against Medical Deep Learning Systems"</div></div>
<div class="paragraph"><p><strong>Can this be reproduced ?</strong></p></div></section><section id="_3_open_datasets"><h2>3 open datasets</h2><div class="ulist"><ul><li><p><a href="https://www.kaggle.com/c/diabetic-retinopathy-detection/data">Diabetic Retinopathy Detection</a></p></li><li><p><a href="http://academictorrents.com/details/557481faacd824c83fbf57dcf7b6da9383b3235a">X-ray Dataset of 14 Common Thorax Disease</a></p></li><li><p><a href="https://www.isic-archive.com/#!/topWithHeader/onlyHeaderTop/gallery">skin cancer dataset</a> (omitted from this presentation)</p></li></ul></div></section><section id="_diabetic_retinopathy_dr"><h2><a href="https://www.kaggle.com/c/diabetic-retinopathy-detection/data">Diabetic Retinopathy</a> (DR)</h2><table class="tableblock frame-all grid-all" style="width:100%"><colgroup><col style="width:35%"><col style="width:65%"></colgroup><tbody><tr><td class="tableblock halign-right valign-middle"><div><div class="paragraph"><p><span class="image"><img src="images/dr-original.png" alt="dr original" width="80%"></span></p></div></div></td><td class="tableblock halign-left valign-middle"><div><div class="ulist"><ul><li><p><a href="https://www.kaggle.com/c/diabetic-retinopathy-detection/data">kaggle dataset</a>: <a href="https://en.wikipedia.org/wiki/Ophthalmoscopy">fundus photographs</a></p></li><li><p>Diabetic retinopathy is the leading cause of blindness in the working-age population of the developed world.</p></li><li><p>Clinicians can identify DR by the presence of lesions (vascular abnormalities due to disease)</p></li><li><p>88702 images, max. 4752x3168 RGB jpegs</p></li><li><p>5 labels (severity of DR)</p></li></ul></div></div></td></tr></table></section><section id="_chestx_ray8"><h2><a href="http://academictorrents.com/details/557481faacd824c83fbf57dcf7b6da9383b3235a">ChestX-ray8</a></h2><table class="tableblock frame-all grid-all" style="width:100%"><colgroup><col style="width:35%"><col style="width:65%"></colgroup><tbody><tr><td class="tableblock halign-right valign-middle"><div><div class="paragraph"><p><span class="image"><img src="images/cxr-original.png" alt="cxr original" width="80%"></span></p></div></div></td><td class="tableblock halign-left valign-middle"><div><div class="ulist"><ul><li><p>Classification and Localization of Common Thorax Diseases</p></li><li><p>112,120 images</p></li><li><p>1024x1024 8-bit greyscale</p></li><li><p>14 labels, appearing non-exclusively</p></li></ul></div></div></td></tr></table></section><section id="_data_access"><h2>Data Access</h2><div class="ulist"><ul><li><p>Diabetic Retinopathy Dataset</p><div class="ulist"><ul><li><p>83 GB shared through kaggle</p></li></ul></div></li><li><p>X-ray Dataset</p><div class="ulist"><ul><li><p>42 GB shared through science torrent</p></li></ul></div></li></ul></div>
<div class="admonitionblock important"><table><tr><td class="icon"><i class="fa fa-exclamation-circle" title="Important"></i></td><td class="content">Very Good!</td></tr></table></div>
<aside class="notes"><div class="ulist"><ul><li><p>download of datasets was somewhat easy</p></li><li><p>kaggle required to create an account</p></li><li><p>omitted dataset used custom formatted REST API without front-end library :/</p></li></ul></div></aside></section><section id="_code"><h2>Code</h2><div class="paragraph"><p><a href="https://github.com/sgfin/adversarial-medicine">github.com/sgfin/adversarial-medicine</a></p></div>
<div class="ulist"><ul><li class="fragment"><p>notebooks reproduce all figures based on downloaded weights</p></li><li class="fragment"><p>python script to retrain two architectures (<a href="https://keras.io/applications/#resnet">Resnet50</a>, <a href="https://keras.io/applications/#inceptionv3">InceptionV3</a>)</p></li></ul></div></section><section id="_code_quality"><h2>Code Quality</h2><div class="ulist"><ul><li class="fragment"><p>code was executable once dropbox weights downloaded</p></li><li class="fragment"><p>notebooks work from top to bottom</p></li><li class="fragment"><p>code largely undocumented</p></li><li class="fragment"><p>paper reports Resnet50 results, weights are for InceptionV3</p></li><li class="fragment"><p>readme reports similar results for either</p></li></ul></div></section></section>
<section><section id="_my_rebuttal"><h2>My Rebuttal</h2></section><section id="_claim"><h2>Claim</h2><div class="quoteblock"><blockquote><div class="paragraph"><p>For each of our representative medical deep learning classifiers, both white and black box attacks were highly successful.</p></div></blockquote><div class="attribution">&#8212; Finlayson et al. "Adversarial Attacks Against Medical Deep Learning Systems"</div></div></section><section id="_white_box_attacks"><h2>White Box Attacks?</h2><div class="paragraph"><p>Given:</p></div>
<div class="stemblock"><div class="content">\$\vec{y} = f(\vec{x}, \theta), L( \vec{y}^{true}, f(\vec{x}, \theta))\$</div></div>
<div class="paragraph"><p>Weight Update Rule:</p></div>
<div class="stemblock"><div class="content">\$\theta_{i+1} = \theta_{i} + \eta \nabla_{\theta}L( \vec{y}^{true}, f(\vec{x}, \theta_i))\$</div></div></section><section id="_projected_gradient_descent_attack"><h2>Projected Gradient Descent Attack</h2><div class="stemblock"><div class="content">\$x^{t+1} = Clip_{x,S} (x^{t} + \epsilon \sign(\nabla_x L(\theta,x,y)))\$</div></div>
<div class="ulist"><ul><li class="fragment"><p>iterate until \(x^{t+1}\) is missclassified</p></li><li class="fragment"><p>For a given set of allowed variations \(S\)</p></li><li class="fragment"><p>PGD <a href="#Madry17">[Madry17]</a>,<a href="#Kurakin1611">[Kurakin1611]</a> is a White Box Attack!</p></li></ul></div>
<aside class="notes"><div class="ulist"><ul><li><p>Using optimisation to counter optimisation</p></li></ul></div></aside></section><section id="_taxonomy_based_on_biggio18"><h2>Taxonomy based on <a href="#Biggio18">[Biggio18]</a></h2><table class="tableblock frame-all grid-all" style="width:100%"><colgroup><col style="width:25%"><col style="width:25%"><col style="width:25%"><col style="width:25%"></colgroup><thead><tr><th class="tableblock halign-left valign-top">type</th><th class="tableblock halign-left valign-top">white box</th><th class="tableblock halign-left valign-top">grey box</th><th class="tableblock halign-left valign-top">black box</th></tr><tbody><tr><td class="tableblock halign-left valign-top"><p class="tableblock">adversary knows</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">all of</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">subset of</p></td><td class="tableblock halign-left valign-top"><p class="tableblock">only queries</p></td></tr><tr><td class="tableblock halign-left valign-top"><div><div class="paragraph"><p>&nbsp;</p></div></div></td><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p>dataset</p></li><li><p>feature set</p></li><li><p>learning alg + loss</p></li><li><p>trained weights</p></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p><span class="graytext">dataset</span></p></li><li><p>feature set</p></li><li><p>learning alg + loss</p></li><li><p><span class="graytext">trained weights</span></p></li></ul></div></div></td><td class="tableblock halign-left valign-top"><div><div class="ulist"><ul><li><p><span class="graytext">dataset</span></p></li><li><p><span class="graytext">feature set</span></p></li><li><p><span class="graytext">learning alg + loss</span></p></li><li><p><span class="graytext">trained weights</span></p></li></ul></div></div></td></tr></table>
<div class="admonitionblock warning"><table><tr><td class="icon"><i class="fa fa-warning" title="Warning"></i></td><td class="content"><span class="XL-text">Boundaries not clear cut!</span></td></tr></table></div></section><section id="_black_box_attacks_assume_knowledge"><h2>Black-Box Attacks assume knowledge</h2><div class="quoteblock"><blockquote><div class="paragraph"><p>[&#8230;&#8203;] black box attacks were performed by crafting the attack against an independently-trained model with the same architecture and then transferring the resultant adversarial examples to the victim.</p></div></blockquote></div></section><section id="_my_motivation"><h2>My Motivation</h2><div class="paragraph"><p><span class="icon"><i class="fa fa-question-circle fa-5x"></i></span></p></div>
<div class="paragraph"><p>How about real black-box attacks?</p></div></section><section id="_foolbox"><h2><a href="https://github.com/bethgelab/foolbox">Foolbox</a></h2><pre class="highlight listingblock"><code data-noescape class="python language-python">import foolbox
import keras
import numpy as np

keras_model = ...#load weights

fmodel = foolbox.models.KerasModel(keras_model, bounds=(0, 255),
				   preprocessing=preprocessing)

image, label = foolbox.utils.imagenet_example()

attack = foolbox.attacks.LinfinityBasicIterativeAttack(fmodel)
adversarial = attack(image[:, :, ::-1], label)</code></pre>
<div class="ulist"><ul><li><p>offers a variety of white and black box attacks</p></li><li><p>simple API can wrap models from TensorFlow, PyTorch, Theano, Keras, Lasagne, MXNet</p></li></ul></div></section><section id="_pure_black_box_on_diabetic_retinopathy"><h2>Pure Black-box on Diabetic Retinopathy</h2><table class="tableblock frame-none grid-none" style="width:100%"><colgroup><col style="width:33.3333%"><col style="width:33.3333%"><col style="width:33.3334%"></colgroup><tbody><tr><td class="tableblock halign-center valign-middle"><div><div class="paragraph text-center"><div class="title">Original</div><p><span class="image"><img src="images/dr-original.png" alt="dr original" width="80%"></span></p></div></div></td><td class="tableblock halign-center valign-middle"><div><div class="paragraph text-center"><div class="title">Adversarial</div><p><span class="image"><img src="images/dr-adversarial.png" alt="dr adversarial" width="80%"></span></p></div></div></td><td class="tableblock halign-center valign-middle"><div><div class="paragraph text-center"><div class="title">Difference*20</div><p><span class="image"><img src="images/dr-diff.png" alt="dr diff" width="80%"></span></p></div></div></td></tr></table>
<pre class="highlight listingblock L-text"><code data-noescape class="source language-source">attack = foolbox.attacks.SaltAndPepperNoiseAttack(fmodel)</code></pre>
<aside class="notes"><div class="ulist"><ul><li><p>real world: shot noise in CCM chip</p></li></ul></div></aside></section><section id="_easy_but_why"><h2>Easy! But why?</h2><table class="tableblock frame-none grid-none" style="width:100%"><colgroup><col style="width:50%"><col style="width:50%"></colgroup><tbody><tr><td class="tableblock halign-left valign-top"><div><div class="paragraph text-center"><div class="title">Class Inbalance</div><p><span class="image"><img src="images/dr_class_overview.svg" alt="dr class overview" width="80%"></span></p></div></div></td><td class="tableblock halign-left valign-top"><div><pre class="highlight listingblock"><code data-noescape class="python language-python">train_generator = train_datagen.flow_from_directory(
            'images/train',
            target_size=(224, 224),
            batch_size=batch_size,
            class_mode='categorical',
            shuffle=True)</code></pre>
<div class="ulist"><ul><li><p>Input image sizes range  [2500-4000]x[2000-2500]</p></li><li><p>before training, resized to 224x224</p></li><li><p>eye doctor: "DR pathologies are very fine structures!"</p></li></ul></div></div></td></tr></table>
<aside class="notes"><div class="ulist"><ul><li><p>inbalanced class frequencies in training dataset</p></li><li><p>real world: DR discovered in very fine structured elements</p></li><li><p>subject to further study</p></li></ul></div></aside></section><section id="_dr_pathologies"><h2>DR Pathologies</h2><table class="tableblock frame-none grid-none" style="width:100%"><colgroup><col style="width:50%"><col style="width:50%"></colgroup><tbody><tr><td class="tableblock halign-center valign-middle"><div><div class="paragraph text-center"><div class="title">Healthy</div><p><span class="image"><img src="images/dr-10-left-level0_800px.jpg" alt="dr 10 left level0 800px" width="80%"></span></p></div></div></td><td class="tableblock halign-center valign-middle"><div><div class="paragraph text-center"><div class="title">Malignant</div><p><span class="image"><img src="images/dr-16-left-level4_800px.jpg" alt="dr 16 left level4 800px" width="80%"></span></p></div></div></td></tr></table></section><section id="_chest_x_ray_dataset"><h2>Chest X-Ray Dataset?</h2><table class="tableblock frame-none grid-none" style="width:100%"><colgroup><col style="width:50%"><col style="width:50%"></colgroup><tbody><tr><td class="tableblock halign-center valign-middle"><div><div class="ulist text-center"><div class="title">No simple attack worked</div><ul><li><p><code>SaltAndPepperNoiseAttack</code></p></li><li><p><code>GaussianBlurAttack</code></p></li><li><p><code>AdditiveUniformNoiseAttack</code></p></li><li><p><code>ContrastReductionAttack</code></p></li></ul></div></div></td><td class="tableblock halign-center valign-middle"><div><div class="paragraph text-center"><div class="title">!</div><p><span class="image"><img src="images/cxr-original.png" alt="cxr original" width="50%"></span></p></div></div></td></tr></table></section><section id="_decision_boundary_attack"><h2>Decision Boundary Attack</h2><div class="paragraph"><p><span class="image"><img src="images/1712.04248_fig7.png" alt="1712.04248 fig7" width="80%"></span></p></div>
<div class="paragraph"><p><a href="#Brendel17">[Brendel17]</a></p></div>
<aside class="notes"><div class="ulist"><ul><li><p>take Dalmatian and visually transform to cat</p></li><li><p>image is classified as Dalmatian</p></li><li><p>high count of calls to model&#8217;s predict function</p></li></ul></div></aside></section><section id="_pure_black_box_attacks_on_chest_x_ray"><h2>Pure Black-box Attacks on Chest X-Ray</h2><table class="tableblock frame-none grid-none" style="width:100%"><colgroup><col style="width:33.3333%"><col style="width:33.3333%"><col style="width:33.3334%"></colgroup><tbody><tr><td class="tableblock halign-center valign-top"><div><div class="paragraph"><p><span class="image"><img src="images/cxr-original.png" alt="cxr original" width="80%"></span>
Original</p></div></div></td><td class="tableblock halign-center valign-top"><div><div class="paragraph"><p><span class="image"><img src="images/cxr-adversarial-template.png" alt="cxr adversarial template" width="80%"></span>
Adversarial</p></div></div></td><td class="tableblock halign-center valign-top"><div><div class="paragraph"><p><span class="image"><img src="images/cxr-adversarial.png" alt="cxr adversarial" width="80%"></span>
Difference*20</p></div></div></td></tr></table>
<aside class="notes"><div class="ulist"><ul><li><p>model&#8217;s predict function was called ~5000 times</p></li></ul></div></aside></section></section>
<section><section id="_summary"><h2>Summary</h2></section><section id="_adversarial_attacks_against_medical_deep_learning_systems_3" class="L-text"><h2>Adversarial Attacks Against Medical Deep Learning Systems</h2><div class="ulist"><ul><li class="fragment"><p>robustness of DL classifiers as diagnostic systems</p></li><li class="fragment"><p>successfully achieved reproducibility (praise to the authors)</p></li><li class="fragment"><p>illustrated classifiers and attacks appear remote to reality</p></li><li class="fragment"><p>portions of the paper contradictory in the details</p></li></ul></div></section><section id="_bigger_picture" class="XL-text"><h2>Bigger Picture</h2><div class="ulist"><ul><li class="fragment"><p>end-to-end DL may not produce machine intelligence (data intelligence?)</p></li><li class="fragment"><p>if data is all you got, better treat it carefully!</p></li><li class="fragment"><p>adversarial examples = tool to probe resilience of your classifiers</p></li></ul></div></section></section>
<section><section id="_references"><h2>References</h2></section><section><div class="ulist bibliography"><ul class="bibliography"><li><p><a></a> Szegedy, Christian, et al. "Going deeper with convolutions." Proceedings of the IEEE conference on computer vision and pattern recognition. 2015, <a href="https://arxiv.org/abs/1409.4842">arXiv:1409.4842</a></p></li><li><p><a></a> Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. "Explaining and harnessing adversarial examples." arXiv preprint <a href="https://arxiv.org/abs/1412.6572">arXiv:1412.6572</a> (2014).</p></li><li><p><a></a> Kurakin, Alexey, Ian Goodfellow, and Samy Bengio. "Adversarial examples in the physical world." arXiv preprint <a href="https://arxiv.org/abs/1607.02533">arXiv:1607.02533</a> (2016).</p></li><li><p><a></a> "Demystifying AI, Machine Learning and Deep Learning", <a href="https://MapRBlog.com/blog/demystifying-ai-ml-dl/">blog entry</a></p></li><li><p><a></a> Finlayson, Samuel G., et al. "Adversarial attacks against medical deep learning systems." arXiv preprint <a href="https://arxiv.org/abs/1804.05296">arXiv:1804.05296</a> (2018).</p></li></ul></div></section><section><div class="ulist bibliography"><ul class="bibliography"><li><p><a></a> Biggio, Battista, and Fabio Roli. "Wild patterns: Ten years after the rise of adversarial machine learning." Pattern Recognition 84 (2018): 317-331. <a href="https://arxiv.org/abs/1712.03141">arXiv:1712.03141</a></p></li><li><p><a></a> Gu, Tianyu, Brendan Dolan-Gavitt, and Siddharth Garg. "Badnets: Identifying vulnerabilities in the machine learning model supply chain." arXiv preprint <a href="https://arxiv.org/abs/1708.06733">arXiv:1708.06733</a> (2017).</p></li><li><p><a></a> Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2017. Towards deep learning models resistant to adversarial attacks.</p></li><li><p>arXiv preprint <a href="https://arxiv.org/abs/1706.06083">arXiv:1706.06083</a> (2017).</p></li><li><p><a></a> Alexey Kurakin, Ian Goodfellow and Samy Bengio. Adversarial Machine Learning at Scale. arXiv preprint <a href="https://arxiv.org/abs/1611.01236">arXiv:1611.01236</a> (2016)</p></li></ul></div></section><section><div class="ulist bibliography"><ul class="bibliography"><li><p><a></a> Jonas Rauber, Wieland Brendel and Matthias Bethge. Foolbox: A Python toolbox to benchmark the robustness of machine learning models. arXiv preprint <a href="https://arxiv.org/abs/1707.04131">arXiv:1707.04131</a> (2017)</p></li><li><p><a></a> Wieland Brendel, Jonas Rauber and Matthias Bethge. Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models. arXiv preprint <a href="https://arxiv.org/abs/1712.04248">arXiv:1712.04248</a> (2017)</p></li></ul></div></section></section></div></div><script src="reveal.js/lib/js/head.min.js"></script><script src="reveal.js/js/reveal.js"></script><script>Array.prototype.slice.call(document.querySelectorAll('.slides section')).forEach(function(slide) {
  if (slide.getAttribute('data-background-color')) return;
  // user needs to explicitly say he wants CSS color to override otherwise we might break custom css or theme (#226)
  if (!(slide.classList.contains('canvas') || slide.classList.contains('background'))) return;
  var bgColor = getComputedStyle(slide).backgroundColor;
  if (bgColor !== 'rgba(0, 0, 0, 0)' && bgColor !== 'transparent') {
    slide.setAttribute('data-background-color', bgColor);
    slide.style.backgroundColor = 'transparent';
  }
})

// See https://github.com/hakimel/reveal.js#configuration for a full list of configuration options
Reveal.initialize({
  // Display presentation control arrows
  controls: true,
  hash: true,
  // Help the user learn the controls by providing hints, for example by
  // bouncing the down arrow when they first encounter a vertical slide
  controlsTutorial: true,
  // Determines where controls appear, "edges" or "bottom-right"
  controlsLayout: 'bottom-right',
  // Visibility rule for backwards navigation arrows; "faded", "hidden"
  // or "visible"
  controlsBackArrows: 'faded',
  // Display a presentation progress bar
  progress: true,
  // Display the page number of the current slide
  slideNumber: 'true',
  // Control which views the slide number displays on
  showSlideNumber: 'all',
  // Push each slide change to the browser history
  history: false,
  // Enable keyboard shortcuts for navigation
  keyboard: true,
  // Enable the slide overview mode
  overview: true,
  // Vertical centering of slides
  center: true,
  // Enables touch navigation on devices with touch input
  touch: true,
  // Loop the presentation
  loop: false,
  // Change the presentation direction to be RTL
  rtl: false,
  // Randomizes the order of slides each time the presentation loads
  shuffle: false,
  // Turns fragments on and off globally
  fragments: true,
  // Flags whether to include the current fragment in the URL,
  // so that reloading brings you to the same fragment position
  fragmentInURL: false,
  // Flags if the presentation is running in an embedded mode,
  // i.e. contained within a limited portion of the screen
  embedded: false,
  // Flags if we should show a help overlay when the questionmark
  // key is pressed
  help: true,
  // Flags if speaker notes should be visible to all viewers
  showNotes: false,
  // Global override for autolaying embedded media (video/audio/iframe)
  // - null: Media will only autoplay if data-autoplay is present
  // - true: All media will autoplay, regardless of individual setting
  // - false: No media will autoplay, regardless of individual setting
  autoPlayMedia: null,
  // Number of milliseconds between automatically proceeding to the
  // next slide, disabled when set to 0, this value can be overwritten
  // by using a data-autoslide attribute on your slides
  autoSlide: 0,
  // Stop auto-sliding after user input
  autoSlideStoppable: true,
  // Use this method for navigation when auto-sliding
  autoSlideMethod: Reveal.navigateNext,
  // Specify the average time in seconds that you think you will spend
  // presenting each slide. This is used to show a pacing timer in the
  // speaker view
  defaultTiming: 120,
  // Enable slide navigation via mouse wheel
  mouseWheel: false,
  // Hides the address bar on mobile devices
  hideAddressBar: true,
  // Opens links in an iframe preview overlay
  // Add `data-preview-link` and `data-preview-link="false"` to customise each link
  // individually
  previewLinks: false,
  // Transition style (e.g., none, fade, slide, convex, concave, zoom)
  transition: 'slide',
  // Transition speed (e.g., default, fast, slow)
  transitionSpeed: 'default',
  // Transition style for full page slide backgrounds (e.g., none, fade, slide, convex, concave, zoom)
  backgroundTransition: 'fade',
  // Number of slides away from the current that are visible
  viewDistance: 3,
  // Parallax background image (e.g., "'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg'")
  parallaxBackgroundImage: '',
  // Parallax background size in CSS syntax (e.g., "2100px 900px")
  parallaxBackgroundSize: '',
  // Number of pixels to move the parallax background per slide
  // - Calculated automatically unless specified
  // - Set to 0 to disable movement along an axis
  parallaxBackgroundHorizontal: null,
  parallaxBackgroundVertical: null,
  // The display mode that will be used to show slides
  display: 'block',

  // The "normal" size of the presentation, aspect ratio will be preserved
  // when the presentation is scaled to fit different resolutions. Can be
  // specified using percentage units.
  width: 1920,
  height: 1080,

  // Factor of the display size that should remain empty around the content
  margin: .05,

  // Bounds for smallest/largest possible scale to apply to content
  minScale: 0.2,
  maxScale: 1.5,

  // Optional libraries used to extend on reveal.js
  dependencies: [
      { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
      { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
      { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
      { src: 'reveal.js/plugin/notes/notes.js', async: true },
      
      
      
      
  ],

  

});</script></body></html>